\documentclass[a4paper,11pt,fleqn,twoside,notitlepage]{report}\usepackage[]{graphicx}\usepackage[]{color}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\setcounter{tocdepth}{3}
\setcounter{secnumdepth}{3}

\usepackage[utf8]{inputenc}
\usepackage[margin=2.5cm]{geometry}
\usepackage{graphicx}
\usepackage[T1]{fontenc}
\usepackage{enumitem}
\usepackage[section]{placeins}
\usepackage{csquotes}
\usepackage[english]{babel}
\usepackage{float}
\usepackage{caption}
\usepackage{makecell}
\usepackage{hyperref}
\usepackage{bm}
\usepackage{newfloat}
\usepackage[linesnumbered,lined,boxed,commentsnumbered]{algorithm2e}
\usepackage{booktabs}
\usepackage{colortbl}
\usepackage{tikz}

\usepackage{amssymb,amsmath,amsthm,amsfonts}
\newtheorem{theorem}{Theorem}

\usepackage{fancyhdr}
\setlength{\headheight}{15pt}

\pagestyle{fancy}
\renewcommand{\chaptermark}[1]{ \markboth{#1}{} }
\renewcommand{\sectionmark}[1]{ \markright{#1}{} }

\fancyhf{}
\fancyfoot[CE,CO]{\thepage}
\fancyhead[LE]{\textit{ \nouppercase{\leftmark}} }
\fancyhead[LO]{\textit{ \nouppercase{\rightmark}} }
%\fancyhead[RE,RO]{Erik Thors\'{e}n, \href{mailto:Ethorsn@gmail.com}{Ethorsn@gmail.com} }
\fancypagestyle{plain}{ %
\fancyhf{} % remove everything
  \renewcommand{\headrulewidth}{0pt} % remove lines as well
  \renewcommand{\footrulewidth}{0pt}
}

\usepackage[citestyle=authoryear,bibstyle=authoryear,natbib=true,backend=bibtex,hyperref=true,maxnames=2]{biblatex}

\bibliography{REFERENCES.bib}

\title{Statistical process control in next-generation sequencing quality control data}
\author{Erik Thors\'{e}n \thanks{Postal adress: Mathematical Statistics, Stockholm University, SE-106 91, Sweden. E-mail: Ethorsn@gmail.com. Supervisors: Taras Bodnar and Johan Dahlberg}}
\date{\today}

\raggedbottom
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}
\maketitle

\begin{center}
\begin{abstract}
Any production process needs to be able to ensure quality output. This is no different to the large area of sequencing where the quality of the sequenced data needs to be assured. Using next generation sequencing (NGS) quality control data and statistical process control we have provided a way to monitor and detect changes in a sequential process. In this thesis we have considered monitoring the mean vector and covariance matrix of transformed quality control data from a Next Generation Sequencing (NGS) machine. The transformed data was assumed to be generated by a multivariate normal distribution. We monitored the mean vector using Hotelling's $T^2$ statistic and Croisers MCUSUM control chart (cf. \citet{HotellingQC}, \citet{Croiser1988}). To monitor the covariance matrix we made use of properties of the singular Wishart distribution, first suggested in \citet{Bodnar2009}. Change-points were estimated using a two-sample version of Hotelling's $T^2$ statistic when Croisers MCUSUM chart gave a indication of a change. This approach was retrospective approach was suggested by \citet{pignatiello2001estimation}. A simulation study was performed to test how the constructed control charts performed under persistent changes. Hotelling's $T^2$ statistic showed poor results while the MCUSUM quickly detected changes in transformed NGS quality control data. The simulations and control charts where migrated to C++ using the Rcpp package together with OpenMP, a parallel programming model, to increase computational power. As a real world application, the control charts were applied to transformed quality control data from machines of the same sort. The constructed control charts was also applied on transformed quality control data from other HiSeq machines and detected structural differences between these in terms of their quality control data. These differences were discovered in the mean and covariance matrix.    

\end{abstract}
\end{center}
\newpage
\section*{Acknowledgements}
I would like to thank the SNP\&SEQ platform for the opportunity of doing my thesis in their facility. I would also like to thank my two supervisors Taras Bodnar and Johan Dahlberg who supported me in the writing of this thesis. Taras, who had to answer all my questions on statistical process control and multivariate statistical analysis and Johan, who had to describe the procedures of sequencing with detailed instructions on the problem. Without their help, this thesis would not have been possible. Last, but not least, I would like to thank the people at the platform who had to answer all my questions regarding the machines and programming. 
\newpage
\tableofcontents
\chapter{Introduction}
\input{./Introduction/IntroThesis.tex}
\section{Outline}
This thesis will be outlined as follows. First, a brief introduction on next generation sequencing (NGS) and the operational routines at the SNP\&SEQ platform is presented. In this section we introduce next generation sequencing, the variables which are collected when the sequencing is performed. Thereafter a chapter presenting the methods to be used in this thesis is presented. Next, a exploratory data analysis is conducted of the two datasets at our disposal. The chapter after that will present results in form of simulations and an application of HiSeq quality control data. We end this thesis with a discussion.
\section{Next generation sequencing and operational routines at the SNP\&SEQ platform}
\input{./NGSmachines.tex}

\chapter{Methods}
%Statistical process control was initially invented by Walter Shewhart in the 1930s, REF12......
\input{./Methods/Methods.tex}

\chapter{Exploratory data analysis}





In this section we will conduct a exploratory data analysis. We will first introduce the datasets which will be used in this thesis and then continue with exploring them seperately. We will focus our attention to the data of one of each model. 

The data to be used in this thesis consists of two sets. The first set contains observations on the lowest level, what we called tag level. There is no fixed number of tags for each run and therefore each flowcell can contain a different number of measurements. In the tag level dataset, there are a total of $786$ runs (unique flowcells) which have been performed since 2012 up until the end of 2015. 

The second dataset contains observations from what we called read level. There are a total of $801$ runs which implies that there is a difference between the datasets. A total of $15$ runs are missing from the Tag level. The missing runs are from the MiSeq 1, HiSeq 3 and 6 machines. These missing runs will be excluded from the data. Also, runs performed in 2012 was done so under different circumstances. It was advised that data from 2012 was not to be used. Therefore, quality control data from 2012 will be removed from our data set.

In Table \ref{CompCycl} we can see the completed run cycles for the HiSeq (Hi) and HiSeqX (HiX) machines. We can start with noticing that HiSeq 1 and 2 are not present in the table. These have been taken out of production. The HiSeqX machines have all been run on the same cycle setting, with every completed cycle equal to 150. This is the only setting used at the SNP\&SEQ platform for HiSeqX. The HiSeq machines shows a much wider range of completed cycles. This is a consequence of the wide range of settings that have been used. We can see that HiSeq 6 (Hi6) have most runs in the vicinity of 124-125 completed cycles. A cycle setting of 126 is one of the most common cycle settings for HiSeq machines at the SNP\&SEQ platform. We will use this machine to represent the HiSeq machines. As the HiSeqX machines did not differ in the cycle setting we will use the HiSeqX 1 to represent the HiSeqX machines. The MiSeq 1 machine has a wide range of 0-500 completed cycles with a lot of different run settings. Those observations having 0 completed cycles are runs which have been documented to be malfunctions. It was removed to shorten the table but will be included, to some extent, in the exploratory analysis.

The last row in Table \ref{CompCycl} shows the total number of runs performed on each machine. The HiSeq 4 machine has most runs of all but also a large diversity in the run settings. 

% latex table generated in R 3.2.3 by xtable 1.8-2 package
% Tue Mar  1 09:34:31 2016
\begin{table}[!t]
\centering
\caption{Table showing the number of flowcells with a specific number of completed cycles for HiSeq (Hi) and HiSeqX (HiX) machine.} 
\begin{tabular}{lccccccccc}
  \toprule 
  & \multicolumn{9}{c}{Machine} \\ \cmidrule(r){2-10} 
Cycles & Hi3 & Hi4 & Hi5 & Hi6 & HiX1 & HiX2 & HiX3 & HiX4 & HiX5 \\ 
  \midrule
49 & 7 &  &  &  &  &  &  &  &  \\ 
  50 & 16 & 10 & 11 & 7 &  &  &  &  &  \\ 
  60 &  &  &  & 3 &  &  &  &  &  \\ 
  99 & 2 & 4 & 2 & 1 &  &  &  &  &  \\ 
  100 & 73 & 49 & 15 & 6 &  &  &  &  &  \\ 
  124 &  & 22 & 22 & 30 &  &  &  &  &  \\ 
  125 &  & 53 & 46 & 50 &  &  &  &  &  \\ 
  150 & 11 & 2 & 14 & 16 & 30 & 27 & 22 & 32 & 16 \\ 
  200 &  &  &  & 1 &  &  &  &  &  \\ 
  250 & 6 & 1 &  &  &  &  &  &  &  \\ 
  \midrule
  $\Sigma$ & 115 & 141 & 110 & 114 & 30 & 27 & 22 & 32 & 16 \\ 
   \bottomrule
\end{tabular}
\label{CompCycl}
\end{table}

We will now investigate investigate the mean q values of each successive run at a tag level. In Figure \ref{fig:TagLevelTS} we can see the mean of mean q tag level measurements together with the range (min to max) for three different machines of different types for lane 1 stratified on read. The observations are presented in their order of appearance.

For lane 1 measurements, the mean for HiSeq 6 of mean q tag level measurements is very connected to its range. If the range is large then the mean is usually worse. The variability of mean tag level measurements in read 2 is larger compared to measurements made in read 1. HiSeqX is seen to have a small range in each run, for read 1 and 2 measurements. Read 2 measurements are lower on average but do not show any substaintial increase in variance. MiSeq 1 is seen to be the worst of all in terms of its mean q tag level measurements. It is clearly seen in the large variance of the means and the large range in some runs. This was to be expected since the MiSeq machine was used for experimental samples on several different settings. 
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[!htbp]
\includegraphics[width=\maxwidth]{figure/TagLevelTS-1} \caption[Figure containing the range (min to max) and mean of each successive run (flowcell)]{Figure containing the range (min to max) and mean of each successive run (flowcell). Here, we are showing read 1 and 2 in lane 1, disregarding what type of setting the run is performed on.}\label{fig:TagLevelTS}
\end{figure}


\end{knitrout}
We will now focus on the HiSeq 6 machine with 102 to 126 completed cycles, in order to make this EDA sufficiently short. 

In Figure \ref{fig:HiSeq6Comb} the range together with the mean of lane 1 and read 1 is shown in their order of appearance. The variables shown here are percent data above a q-score of 30 and the percent tag error. These measurements are from tag level for the HiSeq 6 machine with a cycle setting of 102 to 126. The last figure contains the number of observations in each lane 1 and read 1. %Also, the scale of the percentage is reversed such that both variables graphical interpretation are the same. A low value is not desirable. 
The first variable, the percentage of data above a Q score of 30, which we will refer to as percent Q30, shows a overall small amount of variation. When the spread increases the mean does aswell. The percent tag error can be seen to be very close to zero and at some times equal to zero. This is surprising since the construction of the variable is connected to the error rate. If one is zero, the other should be aswell. However, for \textit{some} runs with zero percent tag error, the error rate is well above zero. We will refrain from using this variable since the quality of it can not be assured. The number of observations contained in lane 1 read 1 varies a lot. It does not seem to relate to the range nor the mean of neither variable.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[!htb]
\includegraphics[width=\maxwidth]{figure/HiSeq6Comb-1} \caption[Figure showing the range (min to max) and mean of each succsessive run (flowcell) in lane 1, read 1, of the two variables Percent q30 and Percentage tag error]{Figure showing the range (min to max) and mean of each succsessive run (flowcell) in lane 1, read 1, of the two variables Percent q30 and Percentage tag error. All runs shown where performed on 126 cycles.}\label{fig:HiSeq6Comb}
\end{figure}


\end{knitrout}
We will now investigate what we called the read level measurements. At this level only one observation per read and lane is supplied. We can therefore consider each run as a realisation of a random vector following a multivariate distribution. In this setting we have 7 different variables, with 16 measurements in each. Since the HiSeq 6 machine has been our main interest so far, we will continue in this fashion and compare it to the other HiSeq machines. All runs will henceforth be using all 8 lanes and a cycle setting on 126. The HiSeq 3 machine do not have any runs on this specific cycle setting and will therefore be omitted.

In Figure \ref{fig:MeanVectorFigure} we have the mean together with the range of the error rate variable. We can see that no measurements are zero in this case. The HiSeq 5 machine seems to have a consistently lower error rate compared to the other machines. The HiSeq 6 machine has shown one, or possibly several, runs with large error rates in different lanes. To further investigate the distribution of the error rate together with those variables which have not been looked upon, we will look at them in a histogram.  
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[!htb]
\includegraphics[width=\maxwidth]{figure/MeanVectorFigure-1} \caption[Mean together with the range of the error rate of each lane and read (lane\_read)]{Mean together with the range of the error rate of each lane and read (lane\_read). Notice that the HiSeq 5 has the lowest mean error rate of all HiSeq machines.}\label{fig:MeanVectorFigure}
\end{figure}


\end{knitrout}


In Figure \ref{fig:ReadlvlER} we have a histograms of the error rate, raw cluster density and the number of raw clusters for lane 1 and 2, both reads. In the error rate (row one), we can see that read 2 contains more variability compared to read 1. The distribution of read 1 is more peaked while the distribution for read 2 is quite flat. We can also see that the distribution is somewhat skewed, tending towards the right. For the two later rows, the density and cluster variable can be seen to be close to symmetric. The distribution of the density and cluster variables also look very much alike. 
%Error rate measurements from read 1 can be stable while read 2 deviates. An example can be seen around the 20th observation in the error rate for lane 1. 
%This deviating behaviour is not necessarily seen in any other variable, in the same lane and read. 
%What can be seen in the second and third row is that the raw- density and cluster variables correlate almost perfectly. 
%This is seen in several variables. The following variables is almost perfectly correlated; raw clusters, post filter clusters, raw density, post filter density for each read inside a lane. 
%We will exclude all but the raw clusters variable in the analysis. 
%The raw cluster variable is also almost perfectly correlated for each read inside a lane. This can be seen in Figure \ref{fig:ReadlvlER} when comparing the variables inbetween reads for a given lane. 
%We will remove read 2 for the raw cluster variable in every lane, from the analysis.  
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[!htb]
\includegraphics[width=\maxwidth]{figure/ReadlvlER-1} \caption[Error rates, the raw density and the number of raw clusters for each read in lanes 1 and 2]{Error rates, the raw density and the number of raw clusters for each read in lanes 1 and 2. The variable name are listed in the following manner: Variable\_lane\_read.}\label{fig:ReadlvlER}
\end{figure}


\end{knitrout}
The Spearman correlation matrix is visualized in Figure \ref{fig:ReadlvlCor}. The number of variables in the Figure is equal to $112$. The correlation matrix is estimated from a sample size of $213$. In this figure the axis labels where omitted but a header for each group of variables is placed next to them. As an example, the top 16 variables in Figure \ref{fig:ReadlvlCor} corresponds to the error rate for each read and lane which is denoted by the label. We will refer to this as a section of variables. 

We can see that the density and cluster sections of variables correlate almost perfectly. This is especially true for measurements on the same read in a lane. The mean q and percent q30 sections seem to be correlated to each other while not having much correlation to the cluster and density variables. The error rate is negatively correlated with percent q30 and mean q measurements from the same read and lane, while not showing much correlation to other reads and lanes. The correlation matrix can almost be placed on a block diagonal form where three first sections of variables create one block and the last four create another. %Variables on the same read and lane seem to be correlated well while 
%A deviating pattern is is also seen in this section of variables. Those mean q and percent q30 corresponding to lane 7 read 1 and 2, shows much less correlation to the variables inside and outside section but shows very strong correlation to eachother. Also, the percent tag error shows correlation between lanes with low lane number (\textit{reformulate?}) but lanes with high lane number show very little correlation to the others. 

For further analysis we will consider the quality control data for HiSeq 6 with variables mean q, error rate and percent q30. We will continue to use a cycle setting of 126. The mean q, error rate can be assumed to have support on the positive real line. These variables can not be assumed to follow a normal distribution and will therefore be transformed. We will use a Box-Cox transformation (cf. \citet{BoxCox}) on these variables and estimate the transformation parameter $\lambda$ using the Guerro method (cf. \citet{Gurrero}). Also, if necessary, we will divide the transformed variables by a constant to change the scale. The percent q30 variable has limited support on $(0,1)$ and will be transformed using the quantile normal function. Before the transformation and estimation of transformation parameters are performed we will remove those runs which are poor. Runs will be classified as poor using todays quality control criterias. 

The transformation methods are more thoroughly presented in the Appendix, section \ref{NormalSection}. In this section, we also assess the assumption of normality for the transformed HiSeq 6 data, for the variables previously mentioned, together with the a short investigation of autocorrelation. For further analysis, we assume that the transformed data of the mean q, error rate and percent q30 variables are generated by a multivariate normal distribution. 
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[!ht]
\includegraphics[width=\maxwidth]{figure/ReadlvlCor-1} \caption[Spearman correlation matrix of HiSeq 6 Read level measurements]{Spearman correlation matrix of HiSeq 6 Read level measurements. Two groupings can be seen in the correlation.}\label{fig:ReadlvlCor}
\end{figure}


\end{knitrout}












\chapter{Results}
This section aims to show how the control charts perform in fictive scenarios and practice. The first section will present the calculated control limits and what parameters were used to calculate these. We will introduce the performance measures to be used in this thesis and show how our control charts perform in two simulated scenarios. The last section includes a application on HiSeq quality control data. The control charts will be constructed from the transformed HiSeq 6 quality control data using the transformed mean q, error rate, percent q30 variables. The transformation of the data is described in the Appendix, section \ref{NormalSection}. 
%For the application of the MCUSUM chart we have excluded the HiSeq 3 since it did not have any runs on the same cycle setting. In the application of the MCUSUM charts on the HiSeq 4 and 5 machines we are assuming that the transformed quality control data is generated by the same model we defined in equation \ref{DefCP}. By using the control charts on quality control data in this fashion we are testing if this assumption is true. 




\section{Calculation of control limits}
The number of variables is equal to $p=48$ and the number of observation in the IC sample is equal to $M=73$. Using $\alpha=0.01$, the control limit of Hotelling's $T^2$ is equal to $337.57$. 

To calculate the control limits for the MCUSUM scheme we use the function \texttt{CalculateControlLimit}, described in section \ref{AVERG}. In this thesis we will use a set of allowance constants $k$ to see how the different control charts act with the use of different allowance constants. The following inputs was used in the calculations of the control limits.
\begin{itemize}
\item Allowance constants $k = \{0.30, 0.40, 0.50\}.$
\item Target in-control average run length ARL$^*_0 = 100$.
\item Maximum number of iterations Nmax= 40 or 20.
\item The number $\epsilon$ was set to $0.05$.
\item The number of simulations was set to $10^5$ 
\end{itemize}
The in-control parameters $\boldsymbol{\mu}_0$ and $\boldsymbol{\Sigma}_0}$ were estimated from the transformed data. The constant $a_0$ was set sufficiently small in each simulation to ensure convergence, often close or equal to zero. The upper limit $b_0$ was tailored for each allowance constant, $k$. For $k=0.3$, $b_0$ was chosen large, equal to $1000$ or $10 000$ since no prior information on the control limit is available. In the next calculation with a larger allowance constant $k$, $b_0$ was chosen close to the calculated control limit in previous step with a smaller allowance constant. In Table \ref{Control} the control limits are listed for the mean and covairance chart with the use of different allowance constants. 

% latex table generated in R 3.2.3 by xtable 1.8-2 package
% Mon Jun 13 16:22:24 2016
\begin{table}[ht]
\centering
\caption{The control limits calculated using the function \texttt{CalculateControlLimit} for a set of allowance constants k.\label{Control}} 
\begin{tabular}{lccc}
\toprule
& \multicolumn{3}{c}{$k$} \\ \cmidrule(r){2-4} 
Type & 0.3 & 0.4 & 0.5 \\ 
\midrule
Mean & 2580.242 & 2100.029 & 1713.218 \\ 
\midrule
Covariance & 382.812 & 226.562 & 116.547 \\
\bottomrule
\end{tabular}
\end{table}
\section{Performance measures and simulation study}
We will consider two different out-of-control scenarios. The first scenario will emulate a broken lane, all measurements on this lane will persistently show worse behaviour. The second case considers odd behaviour in the error rate of lane 1, while every other variable is performing as expected. We assume that these scenarios can manifest itself in the mean and the covariance matrix but we will not consider them at the same time. Next we present the performance measures we are going to use. 

\subsection{Perfromance measures}
To evaluate the performance of the charts, two measures will be used. The ARL$_1$, which is described as the time it takes until we discover the change which is present. It is defined in the same manner as the ARL$_0$, i.e.
\begin{align}
&N^*=\inf \{t \in \mathbb{Z}_+: H_t>h \}& \\\nonumber
&\text{E}[N^*] = \text{E}[\inf \{t \in \mathbb{Z}_+: H_t>h \}].& 
\end{align}
Note that we have removed the subscript $h$ and added a star to distinguish between the ARL$_0$ and ARL$_1$. The ARL$_1$ assumes that the machine breaks as soon as we start to monitor the process. For Hotellings $T^2$ we will set a max ARL$_1$ to 500. If Hotellings $T^2$ control chart does not indicate a change after the observations in the out-of-control scenarios we will say that it failed to detect the change.

Now, to assume that the machine breaks as soon as Phase 2 monitoring begins may not be a very realistic case. The conditional expected delay (cf. \cite{ED}) can be used to emulate more realistic cases where changes occur after some time. The conditional expected delay is defined as 
$$
\text{ED}_{\tau}(N^*) = \text{E}_{\tau}[N^*-\tau+1|N^*\geq \tau]
$$
which allows for shifts at arbitrary times $\tau$. In our simulations we will set $\tau=20$. These expectations will be approximated using monte carlo approximation with $10^5$ simulations.

The performance of the change point estimation model will be evaluated using following performance measure
$$
\bar{D}=\frac{1}{n}\sum_{i=1}^n \text{D}_i=\frac{1}{n}\sum_{i=1}^n \left(\hat{\tau}_i-\tau \right)
$$
which will give a indication how how our estimated change point compares to the true value, on average. Since the change point detection model assumed a fixed sample size we will perform the simulations in the following way: 
\begin{itemize}
\item Simulate $\tau=20$ observations according to our in control parameters.
\item Simulate $\lfloor \text{ED}_{\tau}[N^*] \rfloor$ number of observations for the given scenario, size of change and allowance constant. 
\end{itemize}
Here $\lfloor \cdot \rfloor$ is the floor, i.e. we take the closest lowest integer of the conditional expected delay as the number of simulations for a given scenario and size of change. In the simulation of $\bar{D}$ we will use the result of $k=0.3$. 
We will continue with defining these scenarios and how we are going to simulate the performance measures.
\subsection{Simulation study}
Let the observed process $\mathbf{X}_t$ be ordered in the following manner. The first three variables are the mean q, percent q30 and error rate variables from the first lane and read. The second triplet of variables are from the first lane but the second read and so forth. Define the out-of-control mean vector $\boldsymbol{\mu}_1$ and out-of-control covariance matrix $\boldsymbol{\Sigma}_1$ in the following way
\begin{align}
&\boldsymbol{\mu}_1=\boldsymbol{\mu}_0 +\begin{pmatrix} -\delta_1 \\ -\delta_2 \\ \delta_3 \\ -\delta_1 \\ -\delta_2 \\ \delta_3 \\ 0 \\ \vdots \\ 0 \end{pmatrix} & \;\;\;\;\;\;\; & \boldsymbol{\Sigma}_1 = \boldsymbol{\Sigma}_0+\boldsymbol{\Delta} &
\end{align}
where 
$$
\boldsymbol{\Delta}=
\begin{pmatrix} 
\boldsymbol{\Delta}_{1} &\mathbf{0}_{(p-6)\times(p-6)}  \\
\mathbf{0}_{(p-6)\times(p-6)} & \mathbf{0}_{(p-6)\times(p-6)}
\end{pmatrix} &
$$
where $\mathbf{0}_{k\times k}$ is a $k\times k$ matrix with all entries equal to zero. The submatrix $\boldsymbol{\Delta}_{1}$ have dimension $6 \times 6$.   
We will continue with simulating scenario 1 where we assume that quality control data indicates increasingly bad performance in lane 1. 
\subsubsection{Scenario 1 - All quality control variables in lane 1 show increasingly poor behaviour}


In this scenario the quality control data shows persistently worse behaviour. We assume that $\delta_i=\delta>0$ for $i=1,2,3$. We also assume this change manifests in the variance. Therefore, all off-diagonal elemets in $\boldsymbol{\Delta}_1$ are zero and the diagonal elements are equal to a constant $\Delta>0$. These changes will be considered in separate simulation studies. 

In Figure \ref{fig:ARL1MeanCase1} we can see the ARL$_1$ and ED of the MCUSUM for different changes in the mean, for different sizes of $\delta$. Note that the values of $\delta$ are small, this is a result of the scale of the transformed data. The ARL$_1$ goes down quickly for increasing values of $\delta$. The conditional expected delay (ED) is seen to decrease quicker than the ARL$_1$. The value of the allowance constant $k$ does not seem to impact the ARL$_1$ or the ED in scenario 1 for persistent changes in the mean. Hotelling's $T^2$ statistic showed no indication of a change, the smallest out-of-control ARL for all $\delta$ was equal to $500$ for this simulated scenario.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[!ht]
\includegraphics[width=\maxwidth]{figure/ARL1MeanCase1-1} \caption[Out-of-control ARL and ED for the MCUSUM chart of Scenario 1 - changes in the mean of all variables of lane one]{Out-of-control ARL and ED for the MCUSUM chart of Scenario 1 - changes in the mean of all variables of lane one.}\label{fig:ARL1MeanCase1}
\end{figure}


\end{knitrout}


In Table \ref{LongTable1} we have the simulated ARL$_1$ and ED for persistent changes in the covariance matrix. For small changes in the variance of the covariance matrix it takes a long time, on average, to discover the change. As $\Delta$ grows the MCUSUM chart for the covariance matrix detects changes faster. For this specific scenario and shifts the allowance constant $k$ seem to be optimal at $0.5$. The ARL$_1$ and ED is smaller for a allowance constant equal to $0.5$ compared to a allowance constant equal to $0.3$ or $0.4$.
\begin{table}[ht]
\centering
\caption{Scenario 1. MCUSUM simulated out-of-control ARL and ED for changes in covariance matrix. Each cell is on the following form: ARL$_1$ (ED). The ARL$_0$ is equal to 100. $10^5$ replications was used in this simulation study.\label{LongTable1}}
\begin{tabular}{lccccc}
\toprule
& \multicolumn{5}{c}{$\Delta$} \\ \cmidrule(r){2-6}
$k$ & 0.01 & 0.1325 & 0.255 & 0.3775 & 0.5 \\[0.1cm]
\midrule
 0.3 & 92.84 (72.00) & 59.59 (43.69) & 30.55 (23.19) & 18.84 (11.49) & 13.72 (6.84) \\[0.1cm]
\midrule
  0.4 & 94.18 (72.23) &  57.54 (45.90) & 25.71 (21.32) & 14.49 (8.97) & 10.87 (4.44) \\[0.1cm]
\midrule
  0.5 & 87.16 (76.87) & 48.35 (39.75) & 18.11 (12.80) & 10.44 (3.47) & 7.85 (1.10) \\[0.1cm] 
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Scenario 2 - The error rate of lane 1 shows increasingly poor behaviour}

In this scenario we would like to investigate how the charts behave in a situation where only two variables in a lane is effected by some unknown change. In this case we assume that $\delta_1=\delta_2=0$ and $\delta_3>0$. This implies that the error rate increase on average, for lane 1 quality measurements. In the case of changes in the covariance matrix, we assume that the variance will increase and that covariance is held constant. All elements in the matrix $\Delta_1$ are zero except for the third and sixth diagonal element. In Figure \ref{fig:ARL1meanCase2} we can see the ARL$_1$ and ED for the mean in scenario 2. Note that the values of $\delta$ are larger compared to scenario 1. In comparison to scenario 1, it takes a relatively large $\delta$ to discover a change in the error rate variables of lane 1. Hotelling's $T^2$ statistic did not show any indication of a change, the smallest out-of-control ARL for all $\delta$ was equal to $500$.

In table \ref{LongTable2} the results for the ARL$_1$ and ED for simulated changes in the covariance matrix. Here, the value of the allowance constant is seen to have a impact on detecting changes in variance structure of the covariance matrix. The optimal choice of $k$ is equal to $0.5$ for this specific scenario and size of $\Delta$. 

In Table \ref{OffsetTable} we can see the average offset of the change-point estimation procedure for scenario 1 and 2. For increasing values of $\delta$ the estimation procedure becomes more accurate, on average. For small values of $\delta$ we overestimate the change-point. For large values of $\delta$ we start to underestimate the change-point. It should be noted that in cases of large and small $\delta$ we have a very unbalanced sample. 

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[!ht]
\includegraphics[width=\maxwidth]{figure/ARL1meanCase2-1} \caption[Out-of-control ARL simulations of Scenario 2 - changes in the mean of the error rate of lane one]{Out-of-control ARL simulations of Scenario 2 - changes in the mean of the error rate of lane one.}\label{fig:ARL1meanCase2}
\end{figure}


\end{knitrout}



\begin{table}[ht]
\centering
\caption{Scenario 2. MCUSUM simulated out-of-control ARL and ED for changes in covariance matrix. Each cell is on the following form: ARL$_1$ (ED). The ARL$_0$ is equal to 100. $10^5$ replications was used in this simulation study.\label{LongTable2}}
\begin{tabular}{lccccc}
\toprule
& \multicolumn{5}{c}{$\Delta$} \\ \cmidrule(r){2-6}
$k$ & 0.05 & 0.6625 & 1.275 & 1.8875 & 2.5 \\[0.1cm] 
\midrule
 0.3 & 94.45 (74.05) & 70.15 (51.24) & 49.53 (34.93) & 35.46 (23.63) & 26.87 (16.88) \\[0.2cm]
\midrule
  0.4 & 95.46 (73.17) & 68.24 (49.39) & 46.28 (31.53) & 30.98 (19.90) & 22.21 (12.89) \\[0.2cm] 
\midrule
 0.5 & 88.29 (66.32) & 58.54 (42.01) & 37.56 (23.46) & 22.31 (12.33) & 15.44 (6.60) \\[0.2cm] 
\bottomrule
\end{tabular}
\end{table}

\begin{table}[ht]
\centering
\caption{Table containing $\bar{D}$ for both scenarios. 20 in control observations was used together with a allowance constant of $0.3$.\label{OffsetTable}}
\begin{tabular}{lcccccccccc}
\toprule
\textbf{Sc. 1} & \multicolumn{10}{c}{$\delta$} \\ \cmidrule(r){2-11} 
 & 0.0001 & 0.0052 & 0.0103 & 0.0164 & 0.0215 & 0.0276 & 0.0327 & 0.0388 & 0.0439 & 0.0500 \\ 
 $\lfloor ED \rfloor$ & 76 & 7 & 3 & 2 & 2 & 1 & 1 & 1 & 1 & 1 \\
 $\bar{D}$ & 42.63 & 6.43 & 0.29 & -0.06 & -0.21 & -0.24 & -0.41 & -0.88 & -1.77 & -1.63 \\[0.1cm]
\midrule
\textbf{Sc. 2} & \multicolumn{10}{c}{$\delta$} \\ \cmidrule(r){2-11} 
 & 0.05 & 0.27 & 0.48 & 0.70 & 0.92 & 1.13 & 1.35 & 1.57 & 1.78 & 2.00 \\
 $\lfloor ED \rfloor$ & 65 & 23 & 14 & 10 &  8 &  7  & 6  & 5 & 5 & 4 \\
$\bar{D}$ & 41.65 & 5.60 & 0.20 & -0.04 & -0.13 & -0.17 & -0.31 & -0.75 & -1.64 & -1.53 \\
\bottomrule
\end{tabular}
\end{table}
% \subsection{Robustness (optional section)}
% \begin{enumerate}
% \item What happens under the t-distribution or maybe something skew? 
% \item What happens when the mean and/or covariance matrix is wrongly estiamted? How does this effect control limits etc? 
% \end{enumerate}

\section{A application on HiSeq quality control data.}





In this section we will test the control charts which were constructed from transformed quality control HiSeq 6 on the three other HiSeq machines, namely HiSeq 3, 4 and 5. First, we transform the quality control data from HiSeq 3, 4 and 5 using the Box-Cox transformation with the estimated parameters from the HiSeq 6 data. For variables with limited support, we use the quantile normal function to transform the data. After the transformation we test the constructed control charts on the new data. Any alarm that is given will only be a indication that the in-control parameters for HiSeq 6 does not fit the other machines. 
The HiSeq 3 machine has no runs on the same type of setting which we used for estimating our HiSeq 6 in-control parameters. The data from the HiSeq 3 machine is performed on a mixture of settings. The HiSeq 4 and 5 machines have runs performed on the same setting as those on HiSeq 6.



We will use a allowance constant $k=0.3$ for both MCUSUM charts. The control limits can be seen in Table \ref{Control} for the mean and covariance chart, respectively.

In Figure \ref{fig:HiSeqPhase2Hotelling} we can see Hotellings $T^2$ statistic of the transformed HiSeq 3, 4 and 5 quality control data. Here, a $\alpha=0.01$ was used for calculating Hotelling's $T^2$ statistic control limit, defined in \eqref{HotControlLimit}. Hotelling's control limit was calculated to $337.57$. First, notice the scale of the y-axis. The HiSeq 3 machine was run under different settings which is clearly seen in the left figure of Figure \ref{fig:HiSeqPhase2Hotelling}. Almost all observations are well above the control limit. The first run of HiSeq 3, where Hotellings $T^2$ shows a value of $\ensuremath{2.071688\times 10^{4}}$, stands out in terms of great quality measurements in lane 7 while having very poor quality measurements in read 2, lane 5 and lane 8. The majority of Hotelling's $T^2$ statistics based on transformed HiSeq 4 and 5 quality control data are below the control limit. These runs seem to lie close to our estimated in control parameters in the transformed space.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[!ht]
\includegraphics[width=\maxwidth]{figure/HiSeqPhase2Hotelling-1} \caption[Hotellings T-square statistic for the HiSeq 3 (left), HiSeq 4 (middle) and HiSeq 5 machines with in control parameters based on HiSeq 6 data]{Hotellings T-square statistic for the HiSeq 3 (left), HiSeq 4 (middle) and HiSeq 5 machines with in control parameters based on HiSeq 6 data. The straight horizontal line represents the control limit.}\label{fig:HiSeqPhase2Hotelling}
\end{figure}


\end{knitrout}
The MCUSUM charts for the mean vector and covariance matrix are shown in Figure \ref{fig:HiSeq45MCUSUMfig} for the HiSeq 4 and 5 machines. We can see that the charts give a strong indication that the estimated in control mean vector and covariance matrix do not fit the transformed quality data of HiSeq 4 and 5. If we assume that the transformed quality control data for HiSeq 4 and 5 can represent a IC sample from their respective processes, we can calculate the non-centrality parameter, described in section \ref{croiserCUSUM}. Note that we are not removing any observations from the HiSeq 4 and 5 quality control data and that we are using the transformation parameter estimated from HiSeq 6 quality control data. Let $\hat{\boldsymbol{\mu}}_{i}$ be the maximum likelihood estimator of the mean vector based on the $i$-th HiSeq (transformed) quality control data. The non-centrality parameter for HiSeq 4 can be calculated to 
$$
(\hat{\boldsymbol{\mu}}_{0}-\hat{\boldsymbol{\mu}}_{4})\widehat{\Sigma}_0(\hat{\boldsymbol{\mu}}_{0}-\hat{\boldsymbol{\mu}}_{4})'=0.557.
$$
The non-centrality parameter based on HiSeq 5 is calculated to $0.557$. The covariance matrices can be compared using the determinant of the covariance matrices. It represents the squared volume of the parallelotope in $\mathcal{R}^p$ where the eigenvectors are the principal edges (cf. \citet[page 385]{MultStatAnalysis}). The ratio of the determinants serve as a measure of how the squared volume of the parallelotope relates to eachother. Let $\widehat{\Sigma}_i$ be the maximum likelihood estimator based on transformed quality control data from the $i$-th HiSeq machine. Let
$$
R_{i} = \frac{|\widehat{\Sigma}_0|}{|\widehat{\Sigma}_i|}
$$
be the ratio between the in control covariance matrix and the estimated covariance matrix based on the $i$-th HiSeq transformed quality control data. The ratio $R_{i}$ for HiSeq 4 transformed quality control data is equal to $R_{4}=\ensuremath{1.6582958\times 10^{-10}}$ and for HiSeq 5 we have $R_{5}=\ensuremath{1.6582958\times 10^{-10}}$. In the transformed space, these covariance matrices are not equal in terms of their parallelotope volume. Note that this only gives a illustration to whether or not their volume is equal. It does not take the inherent structure of the covariance matrix into account. Also, as described in section \ref{Wishartderiv}, any control chart constructed from the transformed quantities would be sensitive to shifts in the mean. The very large values of the charting statistics, seen in the right column of Figure \ref{fig:HiSeq45MCUSUMfig} could be a result of differences in the covariance matrix \textit{and} the mean vector. Since the charts gave a indication of a change in this manner, the change-point detection model will not be used in this setting.


\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[!ht]
\includegraphics[width=\maxwidth]{figure/HiSeq45MCUSUMfig-1} \caption[MCUSUM control charts monitoring the mean vector and covariance matrix]{MCUSUM control charts monitoring the mean vector and covariance matrix. The in control parameters is based on transformed data from the HiSeq 6 machine. The horizontal line is the control limit calculated with k=0.3.}\label{fig:HiSeq45MCUSUMfig}
\end{figure}


\end{knitrout}


\chapter{Discussion}
\input{./Discussion/Discussion.tex}

\chapter{Appendix}
\section{Hotelling's $T^2$ statistic}\label{HotDerivation}
\input{./Appendix/HotDerivation.tex}
\newpage
\section{Transformation, normal assumption and autocorrelation.}\label{NormalSection}




In this section we will present the transformation methods and evaluate the assumption that the process follows a multivariate normal distribution. We will also look upon the autocorrelation and to what extent it is present in the data. However, the autocorrelation should be interpreted with caution. Not only does the assumption of temporal independence depend upon the normality assumption but it also assumes that the timeseries is regular. 

For the variables which have support on the positive real line we will use the Box-Cox transformation, presented in \citet{BoxCox}, i.e.
$$
Z=
\begin{cases}
\frac{X^{\lambda}-1}{\lambda} & \text{for } \lambda \neq 0 \\
\log(X) & \text{else}.
\end{cases}
$$
for transformation. The variables are transformed independently. The parameter $\lambda$ is estimated using the method suggested in \citet{Gurrero}. The Box Cox transformation and the guerro estimation method is implemented in the \texttt{forecast} package. For variables which have limited support on $(0,1)$ we will use the standard normal quantile function as a transformation method. Consider X having support on (0,1), then we have that
$$
Z = \Phi^{-1}(X),
$$
where Y will follow a normal distribution.

Two statistical tests of the normal assumption are presented in Table \ref{TestTable}, performed on the transformed data. Henze-Zirkler's multivariate test of normality, presented in \citet{HenzeZirkler}, shows some evidence against the null that data is not normally distributed. The genaralized Shapiro-Wilk test of normality, presented in \citet{GenShapWilk}, shows no evidence at all.
% latex table generated in R 3.2.3 by xtable 1.8-2 package
% Fri Jun 17 15:22:38 2016
\begin{table}[ht]
\centering
\begin{tabular}{rlr}
  \hline
 & Test & P.value \\ 
  \hline
1 & Henze-Zirkler's & 0.29 \\ 
  2 & Generalized Shapiro-Wilk & 0.00 \\ 
   \hline
\end{tabular}
\caption{Two statistical tests of normality, Henze-Zirkler's and a generalized Shapiro-Wilk's test. One out of two tests approves of the normality assumption.} 
\label{TestTable}
\end{table}

Under the assumption that the transformed data \textit{is} normally distributed, the autocorrelation may be investigated. If the autocorrelation at a given lag is below the standard normal distributions $95\%$ percentile we can assume that the data is independent in time. There are a total of 1128 correlation coefficients at each lag to investigate. We will show the proportion of lags which are greater than the standard normal distributions $95\%$ percentile and all autocorrelation coefficients, for a given lag, in a histogram. 
For the HiSeq 6 machine, the proportion of autocorrelation coefficients greater than the normal quantile are seen in Table \ref{ACFtable} for lags one through five. The autocorrelation for the first two lags are shown in Figure \ref{fig:FigureACF}. Under the assumption of normally distributed data, it can be seen that the HiSeq 6 data shows to much autocorrelation to be independent in a temporal manner. However, since the transformed data shows little evidence that it is normally disitributed, any conclusions on the independence between observations in a temporal manner, could be highly inapproriate and inaccurate. Also, we have removed data before transformation which may have created or removed autocorrelation. 

We will also test the assumption that the estimated covariance matrix is a positive definite matrix using the \texttt{is.positive.definite} function from the \texttt{matrixcalc} package. 

The result from the function is $TRUE$, the estimated covariance matrix is positive definite. 

% latex table generated in R 3.2.3 by xtable 1.8-2 package
% Fri Jun 17 15:22:38 2016
\begin{table}[ht]
\centering
\begin{tabular}{rr}
  \hline
Lag & Proportion \\ 
  \hline
  1 & 0.21 \\ 
    2 & 0.11 \\ 
    3 & 0.05 \\ 
    4 & 0.04 \\ 
    5 & 0.03 \\ 
   \hline
\end{tabular}
\caption{Proportion of autocorrelation greater than the normal 95 percentile, at lags 1 through 5.} 
\label{ACFtable}
\end{table}

\begin{center}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[!ht]
\includegraphics[width=\maxwidth]{figure/FigureACF-1} \caption[Distribution of autocorrelation coefficients at lag 1 (left) and lag 2 (right) of the transformed data]{Distribution of autocorrelation coefficients at lag 1 (left) and lag 2 (right) of the transformed data.}\label{fig:FigureACF}
\end{figure}


\end{knitrout}
\end{center}
\newpage
\section{Rcpp, OpenMP and Benchmarks}\label{Benchmarks}



In this section we will shortly show some benchmarks of the \textt{SimulateARL0} function implemented in Rcpp, which is used in to simulate the in control ARL for a given control limit and allowance constant. Almost every function used in simulations are implemented in Rcpp. To create benchmarks we will use the following settings 
\begin{itemize}
\item The in control mean vector and covariance matrix
\item A allowance constant equal to $0.3$
\item The control limit $2580.24$
\item The number of threads was held constant, equal to 7. 
\end{itemize}
The number of simulations N=$\{10^2,10^3,10^4,10^5\}$. Each simulation is performed 10 times and the average time is taken as the benchmark for this specific setting. The test system was a Intel\textregistered Core i7-4770S@3.1Ghz with 16Gb system RAM running Ubuntu 14.04.4. In Figure \ref{fig:BenchmarkFigure} we can see how the average time it takes to perform N simulations. The results are shown on a log 10 scale. It takes around $25.56$ minutes to perform a $10^4$ simulations.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[!ht]
\includegraphics[width=\maxwidth]{figure/BenchmarkFigure-1} \caption[Benchmark of the SimulateARL0 function, implemented in Rcpp together with OpenMP]{Benchmark of the SimulateARL0 function, implemented in Rcpp together with OpenMP. }\label{fig:BenchmarkFigure}
\end{figure}


\end{knitrout}
{\Large Present Rcpp code here? Will get very messy!}

%The Rcpp code for the \texttt{SimulateARL0} function is presented below. The functions \texttt{SnewFun} and \texttt{CFun} calculates $S_{t+1}$ and $C_t$ respectively.\textit{THIS GETS VERY MESSY. REMOVE?}

\newpage

\printbibliography
 
\end{document}
