
In thesis we have investigated how the SPC- and change point detection framework can be used to detect and estimate changes in next generation sequencing quality control data. The control charts presented in this thesis were applied on transformed HiSeq quality control data. The performance of these was tested in a simulation study. Two different scenarios were considered. In scenario one we investigated how quick these charts would detect a change in all variables in a lane. In scenario two we tested how quick the control chart would detect a change in one variable in a lane. Hotelling's statistic was shown to be very poor at detecting small and persistent shifts while the two MCUSUM charts was shown to detect small and persistent changes well. In these two scenarios the change point detection model was shown to be efficient in detecting when the change occurred, if the change was large and the number of runs in OC was relatively short. The number of out-of-control observations used in the estimation was determined by the conditional expected delay. 

We also applied the constructed control charts on other similar machines transformed quality control data. In this application, Hotelling's $T^2$ statistic successfully show differences between runs performed on different cycle settings. It also provides a way to specify the quality limits or control limits from a theoretical point of view. What the Hotelling's $T^2$ statistic failed to detect was the structural changes in quality control data between HiSeq machines. However, one should note that we are using the transformation parameters estimated from HiSeq 6 quality control data to transform the other machines quality control data. 

%The example is not a Phase 2 monitoring example since there may be a numerous of reasons why the charts give a indication of a change. There may be a structural difference between the machines IC parameters and therefore, the signals given by the the two CUSUM charts may be a consequence of this structural difference. The  
% A desirable property is that the HiSeq machines perform in the same manner so the investigation provides some understanding to whether or not these machines are different in the mean vector and covariance matrix. 

%We showed that there was autocorrelation in the data. In \citet{AutoCUSUM} the implications of autocorrelation was investigated in the univariate setting. In this article, they showed that a relatively small autocorrelation can heavily impact the run length distribution. A positive autocorrelation would lead to a decrease in the ARL$_0$. Now, in this specific application the autocorrelation may be somewhat misleading. This assumes that the timeseries which we are investigating is observed at regular intervals. This is not the case. Not only are the machines run at irregular times but the different settings implies that the machine may be used in-between our observations. As an example, according to our model the machine is idle between the $\mathbf{X}_t$ and $\mathbf{X}_{t+1}$ observation. This might be the case, but it can also be that the machine is run on a different setting, which provided different characteristics. A solution is to monitor the grand mean of the error rate or mean q, as an example. In this case we would aggregate all values from different lanes and reads, receiving one observation per variable in each flowcell. We could then adjust for what type of setting the run was performed on. However, this would disregard a possible lane and read effect which may be a large loss of information. 

%In the Literature, there exists non-parametric methods which could be used instead of the charts suggested in this thesis. Assumptions such as the target process should be generated by some symmetric distribution are often made (cf. section 8.2.3 \citet{SPCIntro} or \citet{QuiBoot}). Also, the determination of appropriate control limits is not trivial in this setting. \citet{QuiBoot} suggested a bootstrap procedure to determine the control limits. 

%The change-point estimation model was shown to be ...... However, as described in section \ref{CPDsection}, we refrained from ever estimating a change point of the covariance matrix since the change could only be estimated far back in time. In the event of a change which is discovered  The theory only allows us to consider 
%theory which has been found to determine the control limits in these settings are shown to work through simulations.  
%A example of these are  suggested bootstrap based control limits

In some areas of statistical process control, the mean and covariance structure can be assumed to be known. This is especially true for production processes where one can decide the \textit{desired} mean, variance and covariance structure. In the multivariate setting a new issue arises. Specifying a desired variance for a univariate distribution does not pose a issue but specifying the $p(p+1)/2$ elements in a covariance matrix may be very hard, especially if $p$ is large. Also, the transformation used in this thesis provides several complications. Consider the case when the mean vector is known and is specified in the initial parameter space. We assumed that the transformed data is normally distributed and therefore, we would need to transform the known mean to the new parameter space. The transformation was done by using a Box-Cox transformation, where the parameter $\lambda$ was estimated from data. Any estimate contains uncertainty and therefore the transformation is not deterministic. The problem becomes even more complicated if the parameter $\lambda$ would be random. 

In the Appendix, section \ref{NormalSection} the assumption of normality and temporal independence was investigated. The transformed quality control data showed little evidence of being normally distributed. The evidence shown by these statistical tests could perhaps be increased by further investigating transformation methods. Since there was little evidence for the assumption of normally distributed data any conclusions on temporal independence should be done with great care. However, temporal independence is not only a desirable property for the quality control data but should also be considered a necessary assumption. Not only did the data consist of irregular time series, it was made even more irregular from the different run settings a machine could be run on. Each run setting provided different quality characteristics and dimensionality to the flowcells read level. The autocorrelation assumes that the data are regularly spaced time series, which is not the case. Therefore, using the autocorrelation as a measure of temporal dependency is not only misleading but also violates the assumptions it is built upon.
A solution to this problem is to monitor results quality variables at a flowcell level, aggregating each quality measurements for each read and lane to receive one observation per variable. In this setting one could possibly consider the run settings as fixed, regress upon these fixed settings and then monitor the residuals. 

The use of Rcpp (c.f. \citet{Rcpp}) significantly reduced the time it took to perform the simulations in this thesis. The benchmarks, seen in the Appendix section \ref{Benchmarks}, provide a great indication of how fast Rcpp together with OpenMP can be and what they can do for computer intensive methods in the statistical environment \textsf{R}. \textsf{R} provide a trade-off between performance and readability. The programming language C++ do not follow this paradigm. Rcpp tries to combine these two by using the performance of C++ and the syntax of \textsf{R}. However, it should not be denied that Rcpp is significantly harder to read compared to \textsf{R}.

The 
%For future research another approach to the problem could be useful and perhaps even provide better results. The assumption that each lane and read constitute a random variable correlated with the other variables could be a misinterpretation of the problem itself. What could be more appropriate would be to assume that there exists latent variables. The motivation of latent variables could be as follows, consider a flowcell to be sequenced. The flowcell contains a sample which is \textit{one} source of variation. Second, the machine performs all the sequencing procedures which could be seen as the \textit{second} source of variation. Other sources of variation could also be included, such as a ''human component'' which could be described as the variation coming from the human interaction with the samples. One could then run charts such as those described in this thesis on the .....

%{\Large ToDo}
%\begin{itemize}
%\item what where the assumptions, are they violated? 
%\item shortcomings of the methods? Comment on non-parametric methods.
%\item further research? 
%\item CPD detection shortcomings: \cite{CPDall} the generalized inverse together with a generalized determinant was used to overcome these %shortcomings.
%\end{itemize}