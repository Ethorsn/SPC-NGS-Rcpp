In this section we derive the Hotelling's test statistic from the likelihood ratio criterion, as described in \cite{MultStatAnalysis} section 5.2.1. Consider a random sample $\mathbf{X}=\{\mathbf{X}_1, \mathbf{X}_2,...,\mathbf{X}_n\}$ of size $n$, $n>p$, from a p-dimensional normal distribution with mean $\boldsymbol{\mu}$ and covariance matrix $\boldsymbol{\Sigma}$. We want to test the following simple hypothesis
$$
H_0: \boldsymbol{\mu}=\boldsymbol{\mu}_0 \text{  against  } H_1: \boldsymbol{\mu}\neq\boldsymbol{\mu}_0
$$
The likelihood is equal to
$$
L(\boldsymbol{\mu}, \boldsymbol{\Sigma}; \mathbf{X}) = (2\pi)^{-\frac{np}{2}} |\boldsymbol{\Sigma}|^{-\frac{n}{2}} \exp\left(-\frac{1}{2}\sum_{i=1}^n (\mathbf{X}_i-\boldsymbol{\mu})'\boldsymbol{\Sigma}(\mathbf{X}_i-\boldsymbol{\mu})\right)
$$
and therefore the likelihood ratio is equal to
\begin{equation}\label{HotDerivLikRatio}
\lambda = \frac{\max_{\boldsymbol{\Sigma}} L(\boldsymbol{\mu}_0,\boldsymbol{\Sigma)}}{\max_{\boldsymbol{\mu},\boldsymbol{\Sigma}} L(\boldsymbol{\mu},\boldsymbol{\Sigma)}}.
\end{equation} 
Under the null hypothesis the mean vector is known and therefore, the maximum likelihood estimate (MLE) of the covariance matrix is equal to 
$$
\widehat{\boldsymbol{\Sigma}}_0=\frac{1}{n} \sum_{i=1}^{n}(\mathbf{X}_i-\boldsymbol{\mu})(\mathbf{X}_i-\boldsymbol{\mu})'
$$
and under the alternative hypothesis, MLE for mean vector and covariance matrix are given by
\begin{align}
\hat{\boldsymbol{\mu}}&=\bar{\mathbf{X}}=\frac{1}{n} \sum_{i=1}^{n} \mathbf{X}_i&\nonumber \\
\widehat{\boldsymbol{\Sigma}}_1&=\frac{1}{n}\sum_{i=1}^{n}(\mathbf{X}_i-\hat{\boldsymbol{\mu}})(\mathbf{X}_i-\hat{\boldsymbol{\mu}})'= & \label{step1} \\
&=\frac{1}{n}\left(\sum_{i=1}^{n}(\mathbf{X}_i-\boldsymbol{\mu}_0)(\mathbf{X}_i-\boldsymbol{\mu}_0)+n(\bar{\mathbf{X}}-\boldsymbol{\mu}_0)(\bar{\mathbf{X}}-\boldsymbol{\mu}_0)'\right) & \label{step2}
\end{align}
where we have used lemma 3.2.1 from \cite{MultStatAnalysis} to go from equation \eqref{step1} to \eqref{step2}. 
By Lemma 3.2.2, \cite{MultStatAnalysis} page 69, we have that the numerator and denominator in \eqref{HotDerivLikRatio} is equal to
\begin{align*} 
&\max_{\boldsymbol{\Sigma}} L(\boldsymbol{\mu}_0,\boldsymbol{\Sigma)}=(2\pi)^{-\frac{np}{2}} |\widehat{\boldsymbol{\Sigma}}_0|^{-\frac{n}{2}} \exp\left(-\frac{1}{2}pn\right)& \\ 
&\max_{\boldsymbol{\mu},\boldsymbol{\Sigma}} L(\boldsymbol{\mu},\boldsymbol{\Sigma)}=(2\pi)^{-\frac{np}{2}} |\widehat{\boldsymbol{\Sigma}}_1|^{-\frac{n}{2}} \exp\left(-\frac{1}{2}pn\right). & 
\end{align*}
The maximized likelihoods are proportional to each other and therefore the likelihood ratio in equation \eqref{HotDerivLikRatio} reduces to 
\begin{align}
\lambda&=\frac{|\widehat{\boldsymbol{\Sigma}}_0|^{-\frac{n}{2}}}{|\widehat{\boldsymbol{\Sigma}}_1|^{-\frac{n}{2}}}& \nonumber \\
&=\frac{|\mathbf{V}|^{-\frac{n}{2}}}{|\mathbf{V}+n(\bar{\mathbf{X}}-\boldsymbol{\mu}_0)(\bar{\mathbf{X}}-\boldsymbol{\mu}_0)'|^{-\frac{n}{2}}}& \label{HotDerivLikRatioREDUCED}
\end{align}
where $\mathbf{V}=\sum_{i=1}^{n}(\mathbf{X}_i-\boldsymbol{\mu}_0)(\mathbf{X}_i-\boldsymbol{\mu}_0)'$. Before continuing with the derivations we introduce the following Corollary presented in \citet{MultStatAnalysis} page 638

\textbf{Corollary 1.} \textit{For a non-singular matrix }$\mathbf{C}$ \textit{the following holds}
$$
\begin{vmatrix}
\mathbf{C} & \mathbf{y} \\
\mathbf{y}' & 1
\end{vmatrix} = |\mathbf{C}-\mathbf{y}\mathbf{y}'|=
\begin{vmatrix}
1 & \mathbf{y}' \\
\mathbf{y} & \mathbf{C} 
\end{vmatrix} = |\mathbf{C}|(1-\mathbf{y}'\mathbf{C}^{-1}\mathbf{y})
$$
\textit{where }$\mathbf{y}$\textit{ is a p-dimensional column vector.}

Now, let $\mathbf{C}=-\mathbf{V}$ and $\mathbf{y}=\sqrt{n}(\bar{\mathbf{X}}-\boldsymbol{\mu}_0)$, then by Corollary 1 we have that 
$$
|\mathbf{V}+n(\bar{\mathbf{X}}-\boldsymbol{\mu}_0)(\bar{\mathbf{X}}-\boldsymbol{\mu}_0)'| = \begin{vmatrix}
-\mathbf{V} & \sqrt{n}(\bar{\mathbf{X}}-\boldsymbol{\mu}_0) \\
\sqrt{n}(\bar{\mathbf{X}}-\boldsymbol{\mu}_0)' & 1
\end{vmatrix} = |\mathbf{V}|(1+n(\bar{\mathbf{X}}-\boldsymbol{\mu}_0)'\mathbf{V}^{-1}(\bar{\mathbf{X}}-\boldsymbol{\mu}_0)).
$$
Rewrite equation \eqref{HotDerivLikRatioREDUCED} to
\begin{align}
\lambda^{2/n} &= \frac{|\mathbf{V}|}{|\mathbf{V}+n(\bar{\mathbf{X}}-\boldsymbol{\mu}_0)(\bar{\mathbf{X}}-\boldsymbol{\mu}_0)'|} \nonumber \\ 
&= \frac{|\mathbf{V}|}{|\mathbf{V}|(1+n(\bar{\mathbf{X}}-\boldsymbol{\mu}_0)'\mathbf{V}^{-1}(\bar{\mathbf{X}}-\boldsymbol{\mu}_0))}  \nonumber \\
&= \frac{1}{1+n(\bar{\mathbf{X}}-\boldsymbol{\mu}_0)'\mathbf{V}^{-1}(\bar{\mathbf{X}}-\boldsymbol{\mu}_0)}  \nonumber \\
&= \frac{1}{1+T^2/(n-1)} \label{HotDerivLik} 
\end{align}
where 
\begin{align*}
T^2&=(n-1)n(\bar{\mathbf{X}}-\boldsymbol{\mu}_0)'\mathbf{V}^{-1}(\bar{\mathbf{X}}-\boldsymbol{\mu}_0).& 
\end{align*}
The likelihood ratio, equation \eqref{HotDerivLik}, is then compared to a critical region $\lambda\leq\lambda_0$ is chosen such that it results in the desired significance level, under the assumption that the null is true. Since $\lambda_0$ is a real number for us to chose such that the critical region has the right significance level we chose the following 
$$
\lambda_0= \frac{1}{1+T_0^2/(n-1)}\;\; \iff \;\; T_0^2 = \frac{n-1}{ \lambda_0^{-2/n}-1}.
$$
We can now compare Hotelling's $T^2$ statistic to its quantile $T^2_0$. This shows that Hotellings $T^2$ statistic can be derived from a likelihood ratio test. 