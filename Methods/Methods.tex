%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% INTRODUCTION TO METHODS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In this section we will introduce the model, control charts and change-point estimation procedure to be used in this thesis. We begin with introducing our problem, and how we will approach it from a mathematical point of view. After this introduction we continue with the control charts, how to construct them and last the change-point detection procedure.
%As described in \cite{SPCIntro} section 1.3, Phase 1 is primary of exploratory nature. In the literature, it is argued that in Phase 1 one should try to control the process such that it performs optimal or whatever performance that can be considered as in control. In this project, no such thing is possible since the machines at SNP\&SEQ platform cost a lot to run. We will construct a IC sample from all runs performed on these machines up until 2016. From the IC sample we will estimate the IC distributions parameters. 

\section{Problem description}
In Phase 1 we assume that we have observed the target process, which we denote $\{\mathbf{Y}_t\}$. The target process represents what was called the in control state or in control behaviour of the process. It is assumed that realisations from this process are independent and identically distributed according to a $p$-variate normal distribution with mean vector $\boldsymbol{\mu}_0$ and non-singular covariance matrix $\boldsymbol{\Sigma}_0$. The process which generates $\{\mathbf{Y}_t\}$ will be referred to as the in control distribution or in control process. Since independence is assumed we can disregard in which order the observations appear. In this thesis, the parameters are assumed to be unknown and will therefore be estimated using a random sample. Let $\hat{\boldsymbol{\mu}}_0$ and $\widehat{\boldsymbol{\Sigma}}_0$ denote the maximum likelihood estimate based on a random sample $\mathbf{Y}=\{\mathbf{Y}_1,\mathbf{Y}_2,...,\mathbf{Y}_M\}'$ from the in control distribution.

In Phase 2 we consider a sequential p-dimensional process $\{\mathbf{X}_t\}$, which we will refer to as the observed process. If the observed process were to coincide with the target process, that is they share the same distribution, then we will say that the process is in control. On the contrary, if they are do not share the same distribution we refer to the process as being out-of-control. 

We will consider two types of changes, namely transient and persistent changes. Transient changes are those where the observed process shows out of control behaviour but only for a specific observation. The observed process then goes back to the target process. Persistent changes are defined as whenever the observed process departs from the target process and do not return to it. The problem of discovering persistent changes can be placed in a change point framework (cf. \citet{CPDbook}), i.e.
\begin{equation}\label{DefCP}
X_t \sim
\begin{cases} 
\mathcal{N}_p(\boldsymbol{\mu}_0,\boldsymbol{\Sigma}_0), \;\; t<\tau \\
\mathcal{N}_p(\boldsymbol{\mu}_1,\boldsymbol{\Sigma}_1), \;\; t\geq \tau
\end{cases}
\end{equation}
where $\boldsymbol{\mu}_0$, $\boldsymbol{\mu}_1$ are two p-dimensional mean vectors and $\boldsymbol{\Sigma}_0$, $\boldsymbol{\Sigma}_1$ are two non-singular covariance matrices. We will not consider departures from normality. If $\tau<\infty$ then a change occurred at time $\tau$. Thus, up until time $\tau$ the observed process coincides with the target process. In the change point framework, we aim to test \textit{if} as well as \textit{when} a change has occurred.

For these persistent changes, we will consider shifts in the location, i.e. $\boldsymbol{\mu}_0 \neq \boldsymbol{\mu}_1$, and also changes in the covariance matrix of the distribution i.e. $\boldsymbol{\Sigma}_0 \neq \boldsymbol{\Sigma}_1$. However, these changes will not be considered at the same time. Change-points will only be estimated when our models give a indication that the mean has changed.

\section{Statistical process control - SPC}
In this section we will introduce the control charts to be used in this thesis. These are Hotelling's $T^2$ chart and Croisers multivariate cumulative sum (MCUSUM) chart. All charts provide different characteristics and possibilities to detect different behaviour. We will use Hotelling's $T^2$ chart to monitor transient and large changes and MCUSUM to monitor persistent and small changes. 

We will start with introducing Hotelling's $T^2$ statstic for Phase 2 monitoring. Initially, Hotelling's $T^2$ statistic was derived as the generalisation of the univariate one-sample t-test to the multivariate setting where he was first to use the $T^2$ statistic in the multivariate statistical process control (cf. \citet{HotellingQC}). 
%In the multivariate setting, the sample mean vector is compared to the mean vector of a p-dimensional normal distribution. Hotelling's $T^2$ statistic has been investigated thoroughly in the literature  
%These can be known and/or equal or possibly not known nor equal covariance structure. In Phase 2 monitoring we want to test if a new observation comes from the in control distribution with the help of the $T^2$ statistic. 
\subsection{Hotelling's T-square chart}
Let $\mathbf{X}=(\mathbf{X}_1,\mathbf{X}_2,...,\mathbf{X}_M)$ be a random sample from a multivariate normal distribution with mean vector $\boldsymbol{\mu}$ and covariance matrix $\boldsymbol{\Sigma}_0$. We would like to test if the mean vector is equal to $\boldsymbol{\mu}$ is equal to $\boldsymbol{\mu}_0$. The null and alternative hypothesis are 
$$
H_0: \boldsymbol{\mu}=\boldsymbol{\mu}_0 \text{   against   } H_1: \boldsymbol{\mu}\neq\boldsymbol{\mu}_0
$$
Hotelling's $T^2$ statistic is defined as 
$$
T^2 = (M-1)(\bar{\mathbf{X}}-\boldsymbol{\mu}_0)'\widehat{\boldsymbol{\Sigma}}_0^{-1}(\bar{\mathbf{X}}-\boldsymbol{\mu}_0)
$$
where $\bar{\mathbf{X}}$ is the mean vector of $\mathbf{X}$. The statistic can be derived from a likelihood ratio test. The derivation is shown in the Appendix, section \ref{HotDerivation}. In Phase 2 monitoring we observe $\mathbf{X}_t$ one at a time, in a sequential manner. Let $\mathbf{X}_t$ be a observation from the observed process at time point $t$. As presented in \citet{SPCIntro}, section 7.2.2, we Hotelling's $T^2$ statistic for Phase 2 monitoring uses the following charting statistic
$$
T^2_{2,t} = (\mathbf{X}_t-\boldsymbol{\mu}_0)'\boldsymbol{\Sigma}^{-1}_0(\mathbf{X}_t-\boldsymbol{\mu}_0).
$$ 
Under known in control parameters we can interpret the charting statistic $T^2_{2,t}$ as the squared Mahalanobis distance of $\mathbf{X}_t$ to the in control mean vector $\boldsymbol{\mu}_0$, with respect to $\boldsymbol{\Sigma}_0$. Under unknown in control parameters, the in control sample obtained in Phase 1 is used to estimate the in control parameters. By the assumption of independence the in control sample is independent of the observations in Phase 2 monitoring and therefore, the in control parameters are independent of the observed process seen in Phase 2. Hence, the interpretation of $T^2_{2,t}$ does not change. 

Under the assumption that the observed process is in control, $\mathbf{X}_t$ follows the target process. \citet{Tracy1992} showed that under this assumption, 
$$
\frac{(M-p)M}{p(M-1)(M+1)} T^2_{2,t} \sim \mathcal{F}_{p,M-p}
$$
where $\mathcal{F}_{a,b}$ is the F-distribution with parameters $a$ and $b$, $M$ is the sample size of the in control sample and $p$ is the number of parameters. Under a fixed $\alpha$ the control limit $h$ is calculated according to 
\begin{center}
\begin{equation}\label{HotControlLimit}
h=\frac{p(M-1)(M+1)}{(M-p)M}\mathcal{F}_{1-\alpha,p,(M-p)}
\end{equation}
\end{center}
where $\mathcal{F}_{1-\alpha,p,M-p}$ is the $(1-\alpha)\%$ percentile of the $\mathcal{F}_{p,M-p}$ distribution. We signal a alarm if $T^2_{2,t} > h$. 

In the following section we introduce CUSUM chart in the univariate setting and continue on to the multivariate setting. 
\subsection{The cumulative sum (CUSUM) chart}
The univariate CUSUM chart was originally presented by \citet{ESPAGE}. He constructed the CUSUM chart based on what is called the sequential probability ratio test (SPRT). We start with presenting some theory for the ordinary hypothesis testing and then extend it to the SPRT framework. We then introduce the CUSUM chart and show how it is connected to SPRT. 

Let $Z$ denote a continuous random variable distributed according to some distribution $\mathcal{F}$. Let $\mathcal{F}$ have probability density function $f_{\mathcal{F}}$ and $z$ be a realisation from this distribution. As presented in \citet{SeqAnalysis} chapter 1, the regular framework of hypothesis testing considers the null- together with the alternative hypothesis in the following fashion
\begin{align*}\label{nullHyp}
&H_0: Z \sim \mathcal{F}  & \\
&H_1: Z \sim \mathcal{G}. & 
\end{align*}
where $\mathcal{G}$ is some distribution with density $f_{\mathcal{G}}(z)$. The two distributions $\mathcal{F}$ and $\mathcal{G}$ are known and therefore the hypothesis can be tested using a likelihood ratio test. Let 
$$
l(z)=f_{\mathcal{G}}(z)/f_{\mathcal{F}}(z)
$$ 
be the likelihood ratio. Using the single observation $z$ the likelihood ratio can be computed. The value of the likelihood ratio is then compared to a constant $r_1$. If the likelihood ratio, $l(z)$, is larger then the constant $r_1$ we reject the null. If it is less than $r_1$ we fail to reject the null. A sequential probability ratio test introduces a third possibility for intermediate values of $l(z)$. That is, for $r_0<l(z)<r_1$ where $r_0<r_1$, we can neither reject nor fail to reject the null. Intermediate values indicate that we need more information and should therefore continue to observe, or gather, more observations from the process.

Now, let $z_t$ be realisations of the random variable $Z$ at time-point $t$. These realisations are obtained in a sequential order at regular intervals. \citet{ESPAGE} constructed the following charting statistic
\begin{equation}\label{UniCusum}
C_{t}=\max(C_{t-1}+z_t,0)
\end{equation}
where $C_{0}=0$, in order to detect a increase in the mean of the distribution of $Z$. From equation \eqref{UniCusum}, $C_t=0$ when $C_t<\min_{0 \leq i < t} C_i$. Whenever the sequence $C_t$ receives a new minimum, the process resets and starts again from zero. The charting statistic gives a signal of a increase in the mean if $C_{t}>h$ where $h$ is a pre-specified control limit. To see the connection to the SPRT framework we follow \citet{SPCIntro}, section 4.2.4. Assume that $Z$ follows a normal distribution with mean $\mu$ and variance $\sigma^2$. We are interested in testing the hypothesis 
\begin{align*}
&H_0: Z \sim \mathcal{N}(\mu_0,\sigma^2)& \\
&H_1: Z \sim \mathcal{N}(\mu_1,\sigma^2)& 
\end{align*}
or in more compact form $H_0:\mu=\mu_0$ against $H_1: \mu=\mu_1$ where $\mu_0<\mu_1$. Let $\mathbf{z}_t=\{z_1,z_2,...,z_t\}$ represent a random sample of size $t$. The log likelihood ratio can then be written as 
\begin{align}
\log(l(\mathbf{z}_t)) & =  \log(f_{1}(\mathbf{z}_t)/f_{0}(\mathbf{z}_t))  \nonumber \\
					 & = \log\left(\prod_{i=1}^t f_{1}(\mathbf{z}_i)/f_{0}(\mathbf{z}_i) \right) \nonumber \\ 
				     & = \sum_{i=1}^t \log(f_{1}(z_i))-\log(f_{0}(z_i)) \label{loglik}
\end{align}
where the sub-index of the densities $f_i$, $i=0,1$, refers to the distribution under the null and alternative hypothesis, respectively. Using the density of the normal distribution in equation \eqref{loglik} we have that 
\begin{align}
\log(l(\mathbf{z}_t)) &= \sum_{i=1}^t \left( -\frac{(z_i-\mu_1)^2}{2\sigma^2} + \frac{(z_i-\mu_0)^2}{2\sigma^2} \right) \nonumber \\ 
& = \frac{1}{\sigma^2}\sum_{i=1}^t \left(z_i \mu_1-z_i\mu_0-\frac{(\mu_1^2-\mu_0^2)}{2} \right) \nonumber \\
& = \frac{1}{\sigma^2}\sum_{i=1}^t \left(z_i (\mu_1-\mu_0)-\frac{(\mu_1+\mu_0)(\mu_1-\mu_0)}{2} \right)  \nonumber \\
 &= \frac{\mu_1-\mu_0}{\sigma^2}\sum_{i=1}^t \left(z_i-\mu_0 -k\right)
\end{align}
where $k=(\mu_1-\mu_0)/2$. Let 
\begin{align}
\tilde{C}_t	&=\frac{\sigma^2}{\mu_1-\mu_0}\log(l(\mathbf{z}_t))\\ 
			&= \sum_{i=1}^t \left(z_i-\mu_0 -k\right)\\ 
			&= \tilde{C}_{t-1} + (z_t-\mu_0)-k \label{CUSUMUNI}
\end{align}
where $\tilde{C}_0=0$. If we compare the statistic $C_t$, equation \eqref{UniCusum}, to $\tilde{C}_t$, seen in \eqref{CUSUMUNI}, the main difference can be seen in the maximum and the addition of $k$. The maximum can be interpreted as Pages CUSUM chart will never fail to reject the null hypothesis. The CUSUM chart only considers the two options of rejecting the null or what was described as the third option, we need more information. 

A natural extension of the chart suggested in equation \eqref{UniCusum}, under the assumption of normally distributed data, would be to include the constant $k$, which is referred to as the allowance constant (cf. \citet{SPCIntro} section 4.2.2). This implies the following charting statistic
\begin{equation}\label{UniCusum2}
C_{t}=\max(C_{t-1}+z_t-k,0).
\end{equation}
\citet{Moustakides} showed that Pages CUSUM chart is quickest out of all SPRT tests to detect persistent shifts of size $\mu_1-\mu_0=2k$, given a average run length (ARL) and a allowance constant $k$. The in control average run length will be further extended in section \ref{AVERG}. The size of the shifts are seldom known beforehand which implies that $k$ should be chosen such that we detect a \textit{desirable} shift as soon as possible. 

The CUSUM chart was first extended by \citet{Croiser1988} to the multivariate setting from a two-sided univariate chart introduced in \citet{Croiser1986}. We will now introduce Croisers multivariate CUSUM chart presented in 1988. 
\subsubsection{Croisers multivariate CUSUM chart}\label{croiserCUSUM}
The natural and somewhat blunt extension of the univariate CUSUM chart, equation \eqref{UniCusum2}, to the multivariate setting would be to include vector variables in the scheme, i.e. 
\begin{equation}\label{MultCUSUM}
\mathbf{S}_t=\max(\mathbf{S}_{t-1}+(\mathbf{X}_t-\boldsymbol{\mu}_0)-\mathbf{k},0).
\end{equation}
where $\mathbf{X}_t\sim\mathcal{N}_p(\boldsymbol{\mu}_0, \boldsymbol{\Sigma}_0)$. However, deducing which is the largest, a vector or 0, is not trivial nor is it clear how to choose the column vector of allowance constants $\mathbf{k}$. Therefore, \citet{Croiser1988} suggested the following. Consider the vector $\mathbf{k}$, it must have the same direction as $\mathbf{S}_{t-1}+(\mathbf{X}_t-\boldsymbol{\mu}_0)$, or otherwise increasing elements of $\mathbf{k}$ would not shrink $\mathbf{S}_{t-1}+(\mathbf{X}_t-\boldsymbol{\mu}_0)-\mathbf{k}$ towards the zero vector. Also, if we want $\mathbf{k}$ to shrink $\mathbf{S}_{t-1}+(\mathbf{X}_t-\boldsymbol{\mu}_0)$ to $\mathbf{0}$ it will need to have length $k$, i.e. $(\mathbf{k}'\boldsymbol{\Sigma}_0\mathbf{k})^{1/2}=k$. Therefore we set
$$
\mathbf{k}=(k/C_t)(\mathbf{S}_{t-1}+(\mathbf{X}_t-\boldsymbol{\mu}_0))
$$
where $C_t$ is the length of $\mathbf{S}_{t-1}+(\mathbf{X}_t-\boldsymbol{\mu}_0)$ w.r.t. $\boldsymbol{\Sigma}_0$, i.e.
$$
C_t=\sqrt{(\mathbf{S}_{t-1}+\mathbf{X}_t-\boldsymbol{\mu}_0)'\boldsymbol{\Sigma}^{-1}_0(\mathbf{S}_{t-1}+\mathbf{X}_t-\boldsymbol{\mu}_0)
}$$
Now that we have constructed $\mathbf{k}$ in such a way that it will shrink the vector $\mathbf{S}_{t-1}+(\mathbf{Z}_t-\boldsymbol{\mu}_0)$ towards the zero vector, the maximum taken in equation \ref{MultCUSUM} can be seen as setting $\mathbf{S}_t=\mathbf{0}$ whenever $C_t\leq k$. Rather than considering the multivariate CUSUM in equation \eqref{MultCUSUM} we can consider the following, let
\begin{align}
&C_t=\sqrt{(\mathbf{S}_{t-1}+\mathbf{X}_t-\boldsymbol{\mu}_0)'\boldsymbol{\Sigma}^{-1}_0
(\mathbf{S}_{t-1}+\mathbf{X}_t-\boldsymbol{\mu}_0)}\label{MCUSUM} &\\ 
&\mathbf{S}_t=\begin{cases}
\mathbf{0} & \text{if} \; C_t \leq k \\
(\mathbf{S}_{t-1}+\mathbf{X}_t-\boldsymbol{\mu}_0)(1-k/C_t) & \text{ otherwise}
\end{cases} \label{MCUSUM2} &
\end{align}
where $k$ is the allowance constant and $\mathbf{S}_0=\mathbf{0}$. Let 
\begin{equation}\label{Ht}
H_t=\sqrt{\mathbf{S}_t'\boldsymbol{\Sigma}^{-1}_0 \mathbf{S}_t},
\end{equation} 
be the charting statistic. The chart gives a signal if $H_t>h$ where $h$ is a pre-specified control limit. 

\citet{Croiser1988} proved that the chart is directional invariant. It only depends on the non-centrality parameter
$$
\lambda = (\boldsymbol{\mu}_1-\boldsymbol{\mu}_0)'\boldsymbol{\Sigma}_0(\boldsymbol{\mu}_1-\boldsymbol{\mu}_0)
$$
where $\boldsymbol{\mu}_1$ is the out-of-control process mean vector and $\boldsymbol{\mu}_0$ the IC mean vector, defined in equation \eqref{DefCP}. The non-centrality can be interpreted as the statistical distance of the new mean $\boldsymbol{\mu}_1$ to the in control mean. Also, the result implies that we only need to use one chart to monitor all possible changes in the mean vector $\boldsymbol{\mu}_0$. The allowance constant $k$ can be chosen in the same way as described in previous section. 

The choice of $h$ is not trivial and will be extended upon more thoroughly in the section \ref{AVERG}. 

In the next section we will introduce one method for monitoring the covariance matrix, introduced in \citet{Bodnar2009}. They constructed numerous charts for monitoring the covariance matrix based on properties of the singular Wishart distribution.
\subsubsection{Monitoring the covariance matrix using properties of the singular Wishart distribution}\label{Wishartderiv}
% First let us introduce the Wishart distribution together with the singular Wishart distribution. 
Let $\mathbf{X}=(\mathbf{X}_1,\mathbf{X}_2,...,\mathbf{X}_n)'$ be a random sample of size $n$ from $\mathcal{N}_p(\boldsymbol{\mu}_0,\boldsymbol{\Sigma}_0)$. Without loss of generality we may assume that $\mathcal{N}_p(\mathbf{0},\boldsymbol{\Sigma}_0)$. The dimensions of $\mathbf{X}$ is equal to $n \times p$, $n>p$. Let $\mathbf{V}=\mathbf{X}\mathbf{X}'$, then $\mathbf{V}$ follows a p-dimensional Wishart distribution with $n$ degrees of freedom. The $p$ dimensional Wishart distribution, which we will denote $W_p(n,\boldsymbol{\Sigma}_0)$ has the following probability density function (cf. \citet{StatDists}, chapter 47)
\begin{equation}\label{wishartDens}
f(\mathbf{V};\boldsymbol{\Sigma}_0)=\frac{\exp\left(-\frac{1}{2}\mathbf{tr}(\boldsymbol{\Sigma}^{-1}_0\mathbf{V})\right) |\mathbf{V}|^{(n-p-1)/2}}{\Gamma_p(n/2) |2\boldsymbol{\Sigma}_0|^{n/2}}
\end{equation}
where $|\mathbf{A}|$ is the determinant of the matrix $\mathbf{A}$, $\mathbf{tr}(\mathbf{A})$ is the trace of the matrix $\mathbf{A}$ and $\Gamma_p(g)$ is the p-dimensional gamma function. Consider the case where $n<p$, then $\mathbf{V}$ is a rank deficient matrix since
\begin{align*}
&\text{rank}(\mathbf{X})=\min(n,p)=n \; \; &\\
&\text{rank}(\mathbf{V})=\text{rank}(\mathbf{X}\mathbf{X}')\leq \min(\text{rank}(\mathbf{X}),\text{rank}(\mathbf{X}'))=n<p, \;\; \text{with probability one,}&
\end{align*}
by 3.12, \citet{MatrixHandbook}. Also, by the definition of the rank (cf. definition 4.2 \citet{MatrixHandbook}), the matrix $\mathbf{V}$ does not have a inverse and the determinant of the matrix is equal to zero. Under these circumstances, the density in equation \eqref{wishartDens} is zero for all $\mathbf{V}$. The distribution on the other hand, still exists, and do so under the name of the singular Wishart distribution. The properties of the Wishart and singular Wishart distribution was thoroughly investigated in \citet{Bodnar20082389}. These properties was then placed in the multivariate SPC framework in \cite{Bodnar2009}. We will follow their lead in the introduction to monitoring the covariance matrix using properties from the singular Wishart distribution.

 Let $n=1$, then $\mathbf{X}_t$ is a single observation from the p-dimensional observed process. Let $\mathbf{V}_t=\mathbf{X}_t\mathbf{X}_t'$ be the maximum likelihood estimate for the covariance matrix at time point $t$ and partition the matrices $\mathbf{V}_t$ and $\boldsymbol{\Sigma}_0$ in the following way
\begin{align}
\mathbf{V}_t=\begin{pmatrix}
\mathbf{V}_{t;11} & \mathbf{V}_{t;12} \\
\mathbf{V}_{t;21} & \mathbf{V}_{t;22}
\end{pmatrix} & & \boldsymbol{\Sigma}_0=\begin{pmatrix}
\boldsymbol{\Sigma}_{11} & \boldsymbol{\Sigma}_{12} \\
\boldsymbol{\Sigma}_{21} & \boldsymbol{\Sigma}_{22}
\end{pmatrix}.
\end{align}
Note that we removed the subscript 0 when partitioning the covariance matrix. This was done in order to keep the notation readable. Consider the case where $\mathbf{V}_{t;12}$ and $\boldsymbol{\Sigma}_{12}$ are row vectors. For the $i$-th row and column we may reorder $\Sigma_0$ and $\mathbf{V}_t$ such that the element $\boldsymbol{\Sigma}_{11}$ is the $i$-th diagonal element and $\boldsymbol{\Sigma}_{12},\boldsymbol{\Sigma}_{21}$ the $i$-th row and column. Let $\sigma^2_{ii}$ and $\nu_{t,ii}$ be the $i$-th diagonal elements of the covariance matrix $\boldsymbol{\Sigma}_0$ and the matrix $\mathbf{V}_t$, respectively. Let $\boldsymbol{\Sigma}_{21,i}$ and $\mathbf{V}_{t;21,i}$ denote the $i$-th column of $\boldsymbol{\Sigma}_0$ and $\mathbf{V}_{t}$ but without their respective $i$-th element diagonal element. Let $\boldsymbol{\Sigma}_{22,-i}$ denote the $(p-1) \times (p-1)$ matrix without the $i$-th column and row of $\boldsymbol{\Sigma}_0$. The Schur complement (cf. \citet{MatrixHandbook} definition 14.1) for the $i$-th row is defined as
$$
\boldsymbol{\Sigma}^*_{22,-i} = \boldsymbol{\Sigma}_{22,-i} - \boldsymbol{\Sigma}_{21,i}\boldsymbol{\Sigma}_{21,i}'/\sigma^2_{ii}.
$$   
Partition the out-of-control covariance matrix $\Sigma_1$ in equation \eqref{DefCP} in the same manner as above. Let $\Sigma^*_{1;22,-i}$ be the Schur complement for the $i$-th row in the out-of-control scenario. The following theorem was displayed in \citet{Bodnar2009} 
\begin{theorem}
Let $\mathbf{Y}_1, \mathbf{Y}_2,...,\mathbf{Y}_n$ be an i.i.d. $p$ dimensional Gaussian process with $\mathbf{Y}_i\sim \mathcal{N}_p(\mathbf{0},\Sigma_0)$. Let observed process $\{X_t\}$ be defined as
\begin{align}
&X_t \sim
\begin{cases} 
\mathcal{N}_p(\mathbf{0},\boldsymbol{\Sigma}_0), \;\; t<\tau \\
\mathcal{N}_p(\mathbf{0},\boldsymbol{\Sigma}_1), \;\; t\geq \tau.
\end{cases}& \label{obsProc}
\end{align}
Then
\begin{enumerate}[label=(\alph*)]
\item in the in control state 
\begin{equation}\label{etaI}
\boldsymbol{\eta}_{i,t}= \boldsymbol{\Sigma}^*_{22,-i}^{-1/2}\left(\mathbf{V}_{t;21,i}/\nu_{t,ii}-\boldsymbol{\Sigma}_{21,i}/\sigma_{ii} \right)\nu_{t,ii}^{1/2} \sim \mathcal{N}_p(\mathbf{0}_{p-1},\mathbf{I}_{p-1}).
\end{equation}
where $\mathbf{0}_{p-1}$ is the zero vector of length $p-1$ and $\mathbf{I}_{p-1}$ is the $p-1$ dimensional identity matrix.
\item in the out-of-control state 
\begin{align}\label{expect}
& \text{E}[\boldsymbol{\eta}_{i,t}] = (\boldsymbol{\Sigma}^*_{22,-i})^{-1/2} \Omega_i \sigma_ii \frac{\sqrt{2}}{\sqrt{\pi}} & \\
& \text{Var}(\boldsymbol{\eta}_{i,t}) = (\boldsymbol{\Sigma}^*_{22,-i})^{-1/2} \left(\Sigma^*_{1;22,i}+\Omega_i\sigma^2_{ii}(1-2\pi^{-1})\Omega_i' \right)(\boldsymbol{\Sigma}^*_{22,-i})^{-1/2}&
\end{align}
where $\Omega_i = \Sigma_{1;21,i}/\sigma^2_{1;ii}-\boldsymbol{\Sigma}_{21,i}/\sigma_{ii}$.
\end{enumerate}
Moreover, $\{\eta_{i,t}\}$ are independent in the in control and out-of-control state.
\end{theorem}
Part $(a)$ was shown in \citet{Bodnar20082389} and part $(b)$, was shown in \citet{Bodnar2009}. The process $\{\eta_{i,t}\}$ is independent in time if the observed process $\{\mathbf{X}_{t}\}$ is. 

If $\Omega_i=\mathbf{0}_{p-1}$ then no shift has occurred in the covariance matrix of the original observed process $\{\mathbf{X}_{t}\}$. A shift in the covariance matrix would imply a shift in the mean of the transformed quantity in equation \eqref{etaI}. The result provides us with a way to monitor the covariance matrix with methods to monitor changes in the mean of a multivariate normal distribution. Also, if a shift in the mean vector would occur in the observed process in equation \eqref{obsProc} the distribution of $\eta_{i,t}$, $i=1,...,p$ is no longer a $(p-1)$-variate normal distribution. Any control chart which is constructed based on the process $\{\eta_{i,t},\; i=1,...,p\}$ will be sensitive to shifts in the covariance matrix \textit{and} the mean vector of the initial observed process.

In order to monitor the whole covariance matrix we need to use $p$ different charts. As suggested in \citet{Bodnar2009} we define the joint control chart as  
\begin{align}
&C_{i,t}=\sqrt{(\mathbf{S}_{i,t-1}+\boldsymbol{\eta}_{i,t})'(\mathbf{S}_{i,t-1}+\boldsymbol{\eta}_{i,t})} & \\ 
&\mathbf{S}_{i,t}=\begin{cases}
\mathbf{0} & \text{if} \; C_{i,t} \leq k \\
(\mathbf{S}_{i,t-1}+\boldsymbol{\eta}_{i,t})(1-k/C_{i,t}) & \text{ otherwise}
\end{cases} &
\end{align}
where $k$ is the allowance constant and $\mathbf{S}_{i,0}=\mathbf{0}$. Let 
\begin{equation}
H_{i,t}=\sqrt{\mathbf{S}_{i,t}'\mathbf{S}_{i,t}},
\end{equation} 
for $i=1,2,3,...,p$. We define the charting statistic as
$$
H_t = \max(H_{1,t},H_{2,t},...,H_{p,t}).
$$
%where $H_{i,t}$ is the charting statistic based on $\eta_{i,t}$. 
We signal a alarm if $H_t>h$ where $h$ is a pre-specified control limit.

We will now continue with specifying how to determine the control limit $h$ and properly define the average run length. 

%The control limits only need to be determined once, since the random variables $\boldsymbol{\eta}_{i,t}$, $i=1,2,...,p$, are all identically distributed. 

\subsection{Average run length and control limits}\label{AVERG}
In the literature it is often customary to evaluate the performance of the CUSUM or MCUSUM chart using what is called the average run length (ARL). It was first introduced by \citet{ESPAGE} together with the CUSUM chart. It is defined as the average number of observations we can observe in the sequential setting before the chart gives an alarm. Today, literature  differentiate between the in control ARL (ARL$_0$) and the out-of-control ARL (ARL$_1$) (cf. \citet{SPCIntro} or \citet{SPCTomotherapy}). The ARL$_0$ is defined as the average number of observations until the chart gives a alarm when the process is in control. This is closely related to the type 1 error in the regular hypothesis testing framework. The ARL$_1$ represents the average number of observations for the chart to discover a change which is actually present. It is closely related to the power of a test in the regular hypothesis testing framework. 
 
Consider the observed process $\{\mathbf{X}_t\}$ and assume that it is in control. The observed process coincides with the target process. Each observation is assumed to be independent and if the in control parameters are known, the events $H_t > h$, $t=1,2,3,...$, can be seen as independent Bernoulli trials with probability of success $\alpha$. Assume that $h$ is known, $k$ is decided and define the stopping time $N_h$ as the number of independent Bernoulli trials it takes until a successful event, $H_t > h$. The stopping time itself is a random variable which we define as
$$
N_h=\inf\{t\in \mathbb{Z}_+: H_t>h\} \sim \text{ Geo}(\alpha).
$$
The expectation of $N_h$ is by definition equal to the ARL$_0$. If the in control parameters are unknown and estimated from a sample, $N_h$ need not follow a geometric distribution. There will be a dependence between the observations through the estimated parameters. In both cases, to calculate the expectation for a given $h$ is not trivial since the distribution of $H_t$ is not known. However, the distribution need not be known to be able to approximate the expectation of $N_h$. The approximation can be done using Monte Carlo simulation. By the law of large numbers (cf. \citet{LLN} page 172) we have that
$$
\text{E}[N_h] \approx \frac{1}{n} \sum_{i=1}^n N_{h,i}
$$
for large $n$. 

In order to approximate the expectation we need to specify the control limit $h$ and the allowance constant $k$. The Monte Carlo approximation of $\text{E}[N_h]$ using Croisers MCUSUM chart is described in Table \ref{MCAlgo}. The monte carlo approximation is implemented in the functions \texttt{SimulateARL0} and \texttt{SimulateARL0Sigma} which are written in C++ together with OpenMP using the Rcpp extension. OpenMP is a parallell programming model which is written for Fortran, C, C++ and is thoroughly described in \citet{OpenMP}. The Rcpp extension is presented in \citet{Rcpp}. 
\begin{algorithm}
\SetKwInOut{Input}{input}
\SetKwInOut{Output}{output}
\caption{Simulation of the in control average run length, given a control limit $h$ and allowance constant $k$.\label{MCAlgo}}
\SetKwInOut{Initialize}{Initialize}
\Input{A allowance constant $k$, a control limit $h$, the in control parameters $\boldsymbol{\mu}_0$ and $\boldsymbol{\Sigma}_0$}
\Output{A vector with Run lengths}
\BlankLine
\Initialize{$t=0$, $H=0$, $\mathbf{S}_0=\mathbf{0}$}
\While{$H_t<h$ and $t$ is less than some large number}{
Simulate $\mathbf{X}$ from $\mathcal{N}_p (\boldsymbol{\mu}_0, \boldsymbol{\Sigma}_0)$. \\
Calculate $C$ with the help of $\mathbf{S_{t}}$ and $\mathbf{X}$, according to equation \eqref{MCUSUM}. \\
Calculate $\mathbf{S}_t$ according to equation \eqref{MCUSUM2} and then calculate $H_t$. \\
Update $t=t+1$\\
}
Repeat $n$ times.
\end{algorithm}

In order to find the optimal control limit given a target in control average run length, which we refer to as ARL$^*_0$, and a allowance constant $k$, we consider the following function
\begin{equation}\label{FUN}
f(h)=\text{E}[N_h]-ARL^*_0 \approx \left(\frac{1}{n}\sum_{i=1}^{n} N_{i,h}\right)-ARL^*_0.
\end{equation}
We aim to find the $h^*$ which fulfils $f(h^*)=0$. In this thesis, we will use the bisection algorithm described in \citet{NumAnalysis}, page 75, to find such an $h^*$ on a interval $[a_0,b_0]$. In order for the bisection to converge we need to choose a pair $a_0,b_0$ such that $f(a_0)f(b_0)<0$ which can be done by setting $a_0$ relatively small and $b_0$ sufficiently large. However, note that setting $b_0$ large could result in a long time until convergence. The number of simulated run lengths $n$ need to be set equal to a large number to achieve a good estimate of the expectation. The bisection algorithm is implemented in the R function \texttt{CalculateControlLimit}. A outline of the algorithm is presented in Algorithm \ref{hAlgo}. 
\begin{algorithm}
\caption{Bisection algorithm used to find the control limit $h$\label{hAlgo}}
\SetKwData{Left}{left}
\SetKwData{This}{this}
\SetKwFunction{SimulateARL0}{SimulateARL0}
\SetKwInOut{Input}{input}
\SetKwInOut{Output}{output}
\SetKwInOut{Initialize}{Initialize}
\Input{A allowance constant $k$, a interval $[a_0,b_0]$, a target ARL$^*_0$, a maximum number \textbf{Nmax} of iterations, a small number $\epsilon$ and IC parameters $\boldsymbol{\mu}_0$ and $\boldsymbol{\Sigma}_0$}
\Output{Simulated ARL$_0$ for each iteration together with the interval used in that iteration step.}
\BlankLine
	\For{$i \leftarrow 0$ \KwTo \textbf{Nmax}}{
		Set $h_i=(a_i+b_i)/2$ \\
		Use function \texttt{SimulateARL0} or \texttt{SimulateARL0Sigma} to simulate $\text{E}[N_h]$ given $h_{i}$, $k$, $\boldsymbol{\mu}_0$ and $\boldsymbol{\Sigma}_0$.

		\uIf{$|\left(\frac{1}{n}\sum_{i=1}^{n} N_{i,h}\right)-ARL^*_0|<\epsilon$}{
			\textbf{break}, convergence achieved.
		}
		\uElseIf{$\text{ARL}_0<\text{ARL}^*_0$}{
			$a_{i+1}=h_i$ \\
			$b_{i+1}=b_i$
		}
		\Else{
			$a_{i+1}=a_i$ \\
			$b_{i+1}=h_i$		
		}			
	}
\end{algorithm}

In the next section we introduce the change-point detection procedure. The change-point estimation procedure will be used as a retrospective tool for diagnostics when the MCUSUM chart gives a indication of a change. This retrospective change-point estimation procedure was discussed and suggested in the univariate setting by \citet{pignatiello2001estimation}.

\section{Change point estimation}\label{CPDsection}
A CUSUM chart can deliver information indicating that a shift has occurred and also a estimate on when the change occurred. However, \citet{pignatiello2001estimation} proposed the use of a change point detection model to estimate the change point $\tau$, defined in equation \eqref{DefCP}, together with the CUSUM chart. As soon as the CUSUM gave a alarm indicating a change, one would estimate the change point $\tau$ retrospectively. This was done by considering the change point $\tau$ as a parameter of the likelihood and estimating it using maximum likelihood. This proposed method was shown to provide more precise estimates of the change point compared to the estimate from CUSUM charts in a simulation study. 

Consider the model defined in equation \eqref{DefCP}. Let $f_0$ denote the density before the change point $\tau$ and $f_1$ denote the density after the change point. Assume that Croisers CUSUM chart has given a alarm at the $n$th observation in Phase 2 monitoring. Then, we assume that there exists a change point $\tau$ such that $\tau<n$. Let $\mathbf{X}=(\mathbf{X}_1,\mathbf{X}_2,...,\mathbf{X}_n)'$ denote the sample obtained in Phase 2 monitoring. Each observation of the observed process is assumed to be independent and therefore the joint density is equal to
\begin{equation}\label{CPDfun}
f(\mathbf{X},\tau,\boldsymbol{\mu}_1,\boldsymbol{\Sigma}_1) = \prod_{i=1}^{\tau} f_{0}(\mathbf{x}_i;\boldsymbol{\mu}_0,\boldsymbol{\Sigma}_0)\prod_{i=\tau+1}^{n}f_1(\mathbf{x}_i;\boldsymbol{\mu}_1,\boldsymbol{\Sigma}_1).
\end{equation}
The two parameters $\boldsymbol{\mu}_0$ and $\boldsymbol{\Sigma}_0$ are known or have already been estimated and are therefore of no interest to estimate here. We want to estimate the parameters $\tau,\boldsymbol{\mu}_1$ and $\boldsymbol{\Sigma}_1$. First, consider the case where $n<p$. The number of observations obtained in Phase 2 monitoring are less than the number of variables. As described in \citet{CPDbook} section 3.2.1, estimation of the out-of-control covariance matrix $\boldsymbol{\Sigma}_1$ will result in a singular matrix, hence its determinant is equal to zero and by definition the density $f_1(.;\boldsymbol{\mu}_1,\boldsymbol{\Sigma}_1)$ is not defined. Moreover, since the density $f_1(.;\boldsymbol{\mu}_1,\boldsymbol{\Sigma}_1)$ does not exist, the joint density in equation \eqref{CPDfun} does not exist and hence estimation of the change point $\tau$ through a likelihood based procedure is not possible. Now consider the case where $n>p$. In order to estimate a $\boldsymbol{\Sigma}_1$ which must be a non-singular matrix (with probability one) we must consider the case when $p<\tau<n-p$, i.e. the change point would atleast be $p$ steps back in time or atleast $p$ observations past the start of our Phase 2 monitoring. In the case when $p$ is large, the change point can only be estimated far back in time which may not be desirable and in worst case introduce a large bias. Therefore, in this thesis, we will only estimate change points for the mean vector. The density described in equation \eqref{CPDfun} will have parameters $\tau$ and $\boldsymbol{\mu}_1$, i.e.
\begin{equation}\label{CPDfunRed}
f(\mathbf{X};\tau,\boldsymbol{\mu}_1) = \prod_{i=1}^{\tau} f_{0}(\mathbf{X}_i;\boldsymbol{\mu}_0,\boldsymbol{\Sigma}_0)\prod_{i=\tau+1}^{n}f_1(\mathbf{X}_i;\boldsymbol{\mu}_1,\boldsymbol{\Sigma}_0).
\end{equation}
As explained in \citet{CPDbook}, section 3.1.1, we can interpret the situation as a two sample problem. For a given $\tau=\tau_1$, we have two independent samples from two multivariate normal distributions where we would like to test if they share the same mean vector. By this reasoning we can use Hotelling's $T^2$ statistic as a measure of evidence that the change point occurred at the given time point $\tau_1$. The largest value of Hotelling's statistics shows most evidence against the null hypothesis and can be used as a estimator for the change point. 
%We can once more consider the likelihood ratio under the hypothesis 
%$$ 
%H_0: T>n \text{ against } H_1: T<n
%$$
%where the density under the null hypothesis is given by the target distribution and the density under the alternative is given by equation \eqref{CPDfunRed}.
%In  it is suggested to use Hotelling's $T^2$ statistic to estimate the change point $T$.
Hotelling's statistic, derived in Appendix section \ref{HotDerivation}, is done so under the assumption of one sample from a p-dimensional normal distribution. This setting is extended in \citet{MultStatAnalysis}, section 5.3.4 to a two-sample problem where two means from two normal distributions are compared. Here, it is assumed that the covariance matrix are equal but unknown. In this application, the in control parameters are assumed to be known and that some part of the sample may come from the target process. Therefore, we suggest the standardized difference between the in control parameter and the mean of the out of control sample, as done in \citet{MultStatAnalysis} but in the case of a known covariance matrix. The standardized difference is defined as 
$$
\mathbf{y}_{\tau} = \sqrt{\frac{\tau(n-\tau)}{n}}(\boldsymbol{\mu}_0-\hat{\boldsymbol{\mu}}_1(\tau)).
$$
where $\hat{\boldsymbol{\mu}}_1(\tau)=1/(n-\tau)\sum_{i=\tau}^{n} \mathbf{X}_i$ is the maximum likelihood estimator of $\boldsymbol{\mu}_1$ of the density shown in \eqref{CPDfunRed}. We can now construct a two-sample Hotelling's $T^2$ statistic 
\begin{align}
T^2_{\tau}=\mathbf{y}'_{\tau} \boldsymbol{\Sigma}^{-1}_0 \mathbf{y}_{\tau} \;\;\;\;\; \text{for } \tau=1,2,3,...,n-1
\end{align}
and estimate the change point $\tau$ by $T^2_{\hat{\tau}} = \max_{\tau} T^2_{\tau}$.

% As stated in \citet{SomethingOdd}, this estimator maximizes the profile likelihood $L(\tau, \hat{\boldsymbol{\mu}}_1(\tau))$, of the density in \eqref{CPDfunRed}. %This can be shown by considering the maximization of the density in equation \eqref{CPDfunRed} as a linear integer programming problem of the profile likelihood. We will refrain from showing this in this thesis. %The estimated change point can be described as the point in time which maximizes the squared statistical distance between the estimated mean vector $\hat{\boldsymbol{\mu}}_1(T)$ and $\boldsymbol{\mu}_0$.

